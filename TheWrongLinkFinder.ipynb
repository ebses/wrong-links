{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oIWqVJ9QxN2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mport pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "\n",
        "    def clean_data(self):\n",
        "        print(\"Starting data preprocessing...\")\n",
        "        initial_rows = len(self.df)\n",
        "\n",
        "        print(\"Processing Alt Text...\")\n",
        "        self._process_alt_text()\n",
        "\n",
        "        print(\"Removing self-referential links...\")\n",
        "        self._remove_self_links()\n",
        "\n",
        "        print(\"Removing paginated URLs...\")\n",
        "        self._remove_paginated_urls()\n",
        "\n",
        "        print(\"Filtering by type and removing type columni...\")\n",
        "        self._filter_by_type()\n",
        "\n",
        "        print(\"Removing unwanted anchor texts...\")\n",
        "        self._remove_unwanted_anchors()\n",
        "\n",
        "        print(\"Keeping only required columns...\")\n",
        "        self._keep_required_columns()\n",
        "\n",
        "        final_rows = len(self.df)\n",
        "        print(f\"Preprocessing complete. Rows reduced from {initial_rows} to {final_rows}\")\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def _process_alt_text(self):\n",
        "        if 'Alt Text' in self.df.columns:\n",
        "            self.df['Anchor'] = self.df.apply(\n",
        "                lambda row: row['Alt Text'] if pd.isna(row['Anchor']) and not pd.isna(row['Alt Text'])\n",
        "                else row['Anchor'],\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "    def _remove_self_links(self):\n",
        "        self.df['Source_normalized'] = self.df['Source'].str.split('#').str[0]\n",
        "        self.df['Destination_normalized'] = self.df['Destination'].str.split('#').str[0]\n",
        "        self.df = self.df[self.df['Source_normalized'] != self.df['Destination_normalized']]\n",
        "        self.df = self.df.drop(['Source_normalized', 'Destination_normalized'], axis=1)\n",
        "\n",
        "    def _remove_paginated_urls(self):\n",
        "        pagination_patterns = ['\\/page\\/', '\\?p=', 'page=']\n",
        "        pattern = '|'.join(pagination_patterns)\n",
        "\n",
        "        mask = ~(\n",
        "            self.df['Source'].str.contains(pattern, regex=True, na=False) |\n",
        "            self.df['Destination'].str.contains(pattern, regex=True, na=False)\n",
        "        )\n",
        "        self.df = self.df[mask]\n",
        "\n",
        "    def _filter_by_type(self):\n",
        "        if 'Type' in self.df.columns:\n",
        "            self.df = self.df[self.df['Type'].str.lower() == 'hyperlink']\n",
        "            self.df = self.df.drop('Type', axis=1)\n",
        "\n",
        "    def _remove_unwanted_anchors(self):\n",
        "        unwanted_terms = ['login', 'log in', 'register', 'click here', 'next', 'previous']\n",
        "        pattern = '|'.join(map(re.escape, unwanted_terms))\n",
        "        self.df = self.df[~self.df['Anchor'].str.lower().str.contains(pattern, regex=True, na=False)]\n",
        "\n",
        "    def _keep_required_columns(self):\n",
        "        required_columns = ['Source', 'Destination', 'Anchor']\n",
        "        self.df = self.df[required_columns]\n",
        "\n",
        "def analyze_multiple_destinations(df):\n",
        "    \"\"\"Identify all anchor texts that link to multiple destinations\"\"\"\n",
        "    multiple_dest_data = []\n",
        "\n",
        "    # Group by anchor text\n",
        "    anchor_groups = df.groupby('Anchor')\n",
        "\n",
        "    for anchor, group in anchor_groups:\n",
        "        # Get unique destinations\n",
        "        destinations = group['Destination'].unique()\n",
        "        if len(destinations) > 1:\n",
        "            # Count occurrences of each destination\n",
        "            dest_counts = group['Destination'].value_counts()\n",
        "            total_occurrences = len(group)\n",
        "\n",
        "            # Add a row for each destination\n",
        "            for dest, count in dest_counts.items():\n",
        "                multiple_dest_data.append({\n",
        "                    'anchor_text': anchor,\n",
        "                    'destination_url': dest,\n",
        "                    'occurrence_count': count,\n",
        "                    'total_occurrences': total_occurrences,\n",
        "                    'percentage': f\"{(count/total_occurrences)*100:.2f}%\",\n",
        "                    'unique_destinations': len(destinations),\n",
        "                    'example_source': group['Source'].iloc[0]\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(multiple_dest_data)\n",
        "\n",
        "def analyze_inconsistencies_by_most_links(df):\n",
        "    \"\"\"Identify inconsistencies based on most frequently used URLs\"\"\"\n",
        "    inconsistencies = []\n",
        "\n",
        "    # Group by anchor text\n",
        "    anchor_groups = df.groupby('Anchor')\n",
        "\n",
        "    for anchor, group in anchor_groups:\n",
        "        # Get destination counts\n",
        "        dest_counts = group['Destination'].value_counts()\n",
        "        if len(dest_counts) > 1:\n",
        "            # Get the most common destination\n",
        "            most_common_dest = dest_counts.index[0]\n",
        "            most_common_count = dest_counts.iloc[0]\n",
        "\n",
        "            # Find all links that don't point to the most common destination\n",
        "            incorrect_links = group[group['Destination'] != most_common_dest]\n",
        "\n",
        "            for _, row in incorrect_links.iterrows():\n",
        "                inconsistencies.append({\n",
        "                    'source_page': row['Source'],\n",
        "                    'current_destination': row['Destination'],\n",
        "                    'recommended_destination': most_common_dest,\n",
        "                    'anchor_text': anchor,\n",
        "                    'link_count_correct': most_common_count,\n",
        "                    'total_occurrences': len(group),\n",
        "                    'reason': f\"'{anchor}' most commonly links to {most_common_dest} ({most_common_count} times)\"\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(inconsistencies)\n",
        "\n",
        "def analyze_inconsistencies_by_ranking(df, ranking_df):\n",
        "    \"\"\"Identify inconsistencies based on ranking positions\"\"\"\n",
        "    anomalies = []\n",
        "\n",
        "    df['anchor_normalized'] = df['Anchor'].fillna('').str.strip().str.lower()\n",
        "    ranking_df['keyword_normalized'] = ranking_df['Keyword'].fillna('').str.strip().str.lower()\n",
        "\n",
        "    anchor_groups = df.groupby('anchor_normalized')\n",
        "\n",
        "    for anchor, group in anchor_groups:\n",
        "        if not anchor:  # Skip empty anchor texts\n",
        "            continue\n",
        "\n",
        "        ranking_data = ranking_df[ranking_df['keyword_normalized'] == anchor]\n",
        "\n",
        "        if not ranking_data.empty:\n",
        "            best_ranking = ranking_data.sort_values('Ranking Position').iloc[0]\n",
        "            best_ranking_url = str(best_ranking['Top Ranking URL']).rstrip('/')\n",
        "            best_ranking_position = best_ranking['Ranking Position']\n",
        "\n",
        "            # Find all links that don't point to the best ranking URL\n",
        "            incorrect_links = group[group['Destination'].str.rstrip('/') != best_ranking_url]\n",
        "\n",
        "            for _, row in incorrect_links.iterrows():\n",
        "                anomalies.append({\n",
        "                    'source_page': row['Source'],\n",
        "                    'current_destination': row['Destination'],\n",
        "                    'recommended_destination': best_ranking_url,\n",
        "                    'anchor_text': row['Anchor'],\n",
        "                    'ranking_position': best_ranking_position,\n",
        "                    'reason': f\"'{row['Anchor']}' should link to the highest ranking page (position {best_ranking_position})\"\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(anomalies)\n",
        "\n",
        "def run_analysis():\n",
        "    print(\"Internal Link Consistency Analyzer\")\n",
        "    print(\"=================================\")\n",
        "\n",
        "    try:\n",
        "        # Upload crawl data\n",
        "        print(\"1. Please upload your crawl data CSV file.\")\n",
        "        print(\"Required columns: 'Source', 'Destination', 'Anchor', 'Type', 'Alt Text'\")\n",
        "        uploaded_crawl = files.upload()\n",
        "        crawl_file = list(uploaded_crawl.keys())[0]\n",
        "\n",
        "        # Upload ranking data\n",
        "        print(\"2. Please upload your ranking data CSV file.\")\n",
        "        print(\"Required columns: 'Keyword', 'Ranking Position', 'Top Ranking URL'\")\n",
        "        uploaded_ranking = files.upload()\n",
        "        ranking_file = list(uploaded_ranking.keys())[0]\n",
        "\n",
        "        # Load and preprocess crawl data\n",
        "        print(\"Loading and preprocessing crawl data...\")\n",
        "        raw_df = pd.read_csv(crawl_file)\n",
        "        preprocessor = DataPreprocessor(raw_df)\n",
        "        cleaned_df = preprocessor.clean_data()\n",
        "\n",
        "        # Load ranking data\n",
        "        print(\"Loading ranking data...\")\n",
        "        ranking_df = pd.read_csv(ranking_file)\n",
        "\n",
        "        # Generate all three reports\n",
        "        print(\"Generating reports...\")\n",
        "\n",
        "        # 1. Multiple Destinations Report\n",
        "        print(\"1. Analyzing multiple destinations...\")\n",
        "        multiple_dest_df = analyze_multiple_destinations(cleaned_df)\n",
        "        multiple_dest_df.to_csv('multiple_destinations.csv', index=False)\n",
        "        print(f\"Found {len(multiple_dest_df)} instances of multiple destinations\")\n",
        "\n",
        "        # 2. Inconsistencies by Most Links Report\n",
        "        print(\"2. Analyzing inconsistencies by link count...\")\n",
        "        inconsistencies_links_df = analyze_inconsistencies_by_most_links(cleaned_df)\n",
        "        inconsistencies_links_df.to_csv('inconsistencies_by_most_links.csv', index=False)\n",
        "        print(f\"Found {len(inconsistencies_links_df)} inconsistencies based on link count\")\n",
        "\n",
        "        # 3. Inconsistencies by Ranking Report\n",
        "        print(\"3. Analyzing inconsistencies by ranking...\")\n",
        "        inconsistencies_ranking_df = analyze_inconsistencies_by_ranking(cleaned_df, ranking_df)\n",
        "        inconsistencies_ranking_df.to_csv('inconsistencies_by_ranking.csv', index=False)\n",
        "        print(f\"Foundtiple_dest_df.head())\n",
        "\n",
        "        print(\"\\nSample of Inconsistencies by Most Links Report:\")\n",
        "        display(inconsistencies_links_df.head())\n",
        "\n",
        "        print(\"\\nSample of Inconsistencies by Ranking Report:\")\n",
        "        display(inconsistencies_ranking_df.head())\n",
        "\n",
        "        # Download reports\n",
        "        print(\"\\nDownloading reports...\")\n",
        "        files.download('multiple_destinations.csv')\n",
        "        files.download('inconsistencies_by_most_links.csv')\n",
        "        files.download('inconsistencies_by_ranking.csv')\n",
        "\n",
        "        print(\"\\nAnalysis complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        print(\"Full error details:\")\n",
        "        print(traceback.format_exc())\n",
        "        return\n",
        "\n",
        "# Run the analysis\n",
        "run_analysis() {len(inconsistencies_ranking_df)} inconsistencies based on rankings\")\n",
        "\n",
        "        # Show samples of each report\n",
        "        print(\"\\nSample of Multiple Destinations Report:\")\n",
        "        display(mul"
      ],
      "metadata": {
        "id": "b21BbiPyX9cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q5uYp_OpX83n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}